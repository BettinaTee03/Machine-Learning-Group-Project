{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be building our model for the open-world experiments to binary classify by determining whether the web traffic trace corresponds to a monitored website or unmonitored website.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will first import the dataframes into this notebook. Run either 1 of these blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, run this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Use this with colab\n",
    "print(\"Loading datafile...\")\n",
    "with open('datasets/extracted_features.pkl', 'rb') as f:\n",
    "    extracted_df = pickle.load(f)\n",
    "print (\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using local, run this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Use this for local (change the directory to where the extracted_features.pkl is stored on your local machine)\n",
    "# Load the pickle file\n",
    "print(\"Loading datafile...\")\n",
    "# change this directory to the directory where mon_standard.pkl is stored on your local machine\n",
    "file_path = r'C:\\EWHA\\Term 2\\Machine Learning\\pro\\neurotic_networkers\\extracted_features.pkl' # Jordans local path\n",
    "with open(file_path, 'rb') as f: # Path to extracted_features.pkl in Colab\n",
    "    extracted_df = pickle.load(f)\n",
    "print (\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We relabel the data such that monitored websites will be labelled as 1 and unmonitored websites will be labelled as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabelled_df = extracted_df.copy()\n",
    "relabelled_df['label'] = relabelled_df['label'].apply(lambda x: 1 if x>=0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabelled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_initial = relabelled_df.drop(columns=['label'])\n",
    "y_initial = relabelled_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Model with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we construct a model using all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_initial_train, X_initial_test, y_initial_train, y_initial_test = train_test_split(\n",
    "    X_initial, y_initial, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "clf_all_features = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=10, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf_all_features.fit(X_initial_train, y_initial_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the train set\n",
    "y_initial_train_pred = clf_all_features.predict(X_initial_train)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken_all_features_train = end_time - start_time\n",
    "memory_used_all_features_train = process.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_all_features_train, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_all_features_train, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_initial_train, y_initial_train_pred))\n",
    "print(classification_report(y_initial_train, y_initial_train_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_initial_train, y_initial_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_initial_test_pred = clf_all_features.predict(X_initial_test)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken_all_features_test = end_time - start_time\n",
    "memory_used_all_features_test = process.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_all_features_test, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_all_features_test, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_initial_test, y_initial_test_pred))\n",
    "print(classification_report(y_initial_test, y_initial_test_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_initial_test, y_initial_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(y_initial_train)) \n",
    "y_test_bin = label_binarize(y_initial_train, classes=classes)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_proba = clf_all_features.predict_proba(X_initial_train)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Store individual class ROC curves\n",
    "fpr_dict = {}\n",
    "tpr_dict = {}\n",
    "roc_auc_dict = {}\n",
    "\n",
    "# Compute ROC curve for each class\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr_dict[i], tpr_dict[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr_dict[i], tpr_dict[i], \n",
    "        label=f'Class {class_label} (AUC = {roc_auc_dict[i]:.2f})'\n",
    "    )\n",
    "\n",
    "# Compute Macro Average ROC Curve\n",
    "# First, create a grid of FPR points\n",
    "all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(len(classes))]))\n",
    "\n",
    "# Interpolate ROC curves\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(len(classes)):\n",
    "    mean_tpr += np.interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
    "\n",
    "# Average the TPR\n",
    "mean_tpr /= len(classes)\n",
    "\n",
    "# Compute macro-average AUC\n",
    "macro_roc_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "# Plot macro-average ROC curve\n",
    "plt.plot(\n",
    "    all_fpr, mean_tpr, \n",
    "    color='black', \n",
    "    linestyle='--', \n",
    "    linewidth=2, \n",
    "    label=f'Macro-average (AUC = {macro_roc_auc:.2f})'\n",
    ")\n",
    "\n",
    "# Add plot details\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.title('ROC Curve for Website Classification', fontsize=16)\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print macro-average AUC\n",
    "print(f\"Macro-average AUC: {macro_roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_dict = classification_report(y_initial_test, y_initial_test_pred)\n",
    "df = pd.DataFrame(classification_report_dict).T \n",
    "df = df.drop('support', axis=1)  \n",
    "df = df.drop('accuracy', axis=0, errors='ignore')  \n",
    "sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt='.2f')\n",
    "plt.set_title('Heatmap for Classification Report')\n",
    "plt.set_ylabel('Classes')\n",
    "plt.set_xlabel('Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X_initial_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using entropy\n",
    "model_1= RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=10, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "model_1.fit(X_initial_train, y_initial_train)\n",
    "feature_imp_1 = pd.Series(model_1.feature_importances_, index=df_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gini\n",
    "model_2= RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=10, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "model_2.fit(X_initial_train, y_initial_train)\n",
    "feature_imp_2 = pd.Series(model_2.feature_importances_, index=df_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df_X.columns\n",
    "entropy_values = feature_imp_1.values\n",
    "gini_values = feature_imp_2.values\n",
    "\n",
    "bar_width = 0.4 \n",
    "y_positions = range(len(feature_names)) \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "#plotting entropy importance\n",
    "plt.barh(\n",
    "    [y - bar_width / 2 for y in y_positions],\n",
    "    entropy_values,\n",
    "    bar_width,\n",
    "    label='Entropy',\n",
    "    color='blue',\n",
    ")\n",
    "\n",
    "# plotting gini improtance\n",
    "plt.barh(\n",
    "    [y + bar_width / 2 for y in y_positions],\n",
    "    gini_values,\n",
    "    bar_width,\n",
    "    label='Gini',\n",
    "    color='orange',\n",
    ")\n",
    "\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.yticks(y_positions, feature_names) \n",
    "plt.title('Visualisation of Feature Importance: Entropy vs Gini')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the extracted features:\n",
    "> **Feature Group 1: Traffic Volume (Absolute)**  \n",
    "> - Feature 1: Number of incoming packets  \n",
    "> - Feature 2: Number of outgoing packets  \n",
    "> - Feature 3: Total number of packets  \n",
    "> \n",
    "> **Feature Group 2: Traffic Volume (Fraction)**\n",
    "> - Feature 1: Number of incoming packets as a fraction of the total number of packets  \n",
    "> - Feature 2: Number of outgoing packets as a fraction of the total number of packets \n",
    "> \n",
    "> **Feature Group 3: Traffic Ordering List**\n",
    "> - Feature 6: Standard deviation of the outgoing packets ordering list  \n",
    "> - Feature 7: Average of the outgoing packets ordering list  \n",
    "> \n",
    "> **Feature Group 4: Traffic concentration** \n",
    "> - Feature 8: Sum of all items in the alternative concentration feature list  \n",
    "> - Feature 9: Average of all items in the alternative concentration feature list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noted that within each of the 4 feature groups, the features are likely to be highly correlated due to their similarity. Furthermore, feature groups 1 and 2 are closely related as well with 1 being an absolute measurement of traffic volume and 2 as the ratio. Hence we will be selecting 2 features from the combination of group 1 and 2, and 1 feature each from group 3 and 4.\n",
    "\n",
    "According to our feature importance analysis, we have selected the features to be\n",
    "1. Feature 1: Number of incoming packets   \n",
    "2. Feature 3: Total number of packets\n",
    "3. Feature 7: Average of the outgoing packets ordering list \n",
    "4. Feature 8: Sum of all items in the alternative concentration feature list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = relabelled_df[['incoming_packet_counts', 'total_packet_counts', 'avg_outgoing_order', 'sum_concentration']]\n",
    "y = relabelled_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Model with selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we construct an arbitrary random forest classification model using arbitrarily chosen parameters. This section aims to explore the implementation of the model. These parameters will be tuned in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "clf_selected_features = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=10, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf_selected_features.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the train set\n",
    "y_train_pred = clf_selected_features.predict(X_train)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "time_taken_selected_features_train = end_time - start_time\n",
    "memory_used_selected_features_train = process.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_train, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_train, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_test_pred = clf_selected_features.predict(X_test)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "# Make probability predictions on the test set\n",
    "y_test_pred_proba = clf_selected_features.predict_proba(X_test)\n",
    "\n",
    "time_taken_selected_features_test = end_time - start_time\n",
    "memory_used_selected_features_tuned_test = process.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_tuned_test, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_test, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be using Grid Search to tune our model parameters for our Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameter grid for the Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [10, 15],\n",
    "    'min_samples_leaf': [5, 10],\n",
    "    'bootstrap': [True],\n",
    "    'criterion': ['gini', 'entropy']           \n",
    "}\n",
    "\n",
    "# Defining the grid search\n",
    "grid = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42), \n",
    "    param_grid=param_grid, \n",
    "    cv=5,  \n",
    "    refit=True, \n",
    "    verbose = 3,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the grid search\n",
    "start_time = time.time()\n",
    "grid.fit(X_initial_train, y_initial_train)\n",
    "end_time = time.time()\n",
    "time_taken_grid_search = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the time taken to perform the grid search\n",
    "print(\"Time taken for grid search:\", time_taken_grid_search, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters after grid search\n",
    "print(\"Best parameters found:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_initial_train_pred_tuned = grid.predict(X_initial_train)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken_selected_features_tuned_train = end_time - start_time\n",
    "memory_used_selected_features_tuned_train = process.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_tuned_train, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_tuned_train, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_initial_train, y_initial_train_pred_tuned))\n",
    "print(classification_report(y_initial_train, y_initial_train_pred_tuned))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_initial_train, y_initial_train_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_initial_test_pred_tuned = grid.predict(X_initial_test)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken_selected_features_tuned_test = end_time - start_time\n",
    "memory_used_selected_features_tuned_test = process.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_tuned_test, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_tuned_test, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_initial_test, y_initial_test_pred_tuned))\n",
    "print(classification_report(y_initial_test, y_initial_test_pred_tuned))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_initial_test, y_initial_test_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Binarize the test labels\n",
    "classes = grid.classes_\n",
    "y_test_bin = label_binarize(y_initial_test, classes=classes)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_test_proba = grid.predict_proba(X_initial_test)\n",
    "\n",
    "# Function to smooth the ROC curve\n",
    "def smooth_curve(x, y, num_points=3):\n",
    "    # Ensure unique x values\n",
    "    unique_x, indices = np.unique(x, return_index=True)\n",
    "    unique_y = y[indices]\n",
    "    \n",
    "    # Interpolate\n",
    "    f = interp1d(unique_x, unique_y, kind='quadratic', fill_value=\"extrapolate\")\n",
    "    x_new = np.linspace(0, 1, num_points)\n",
    "    y_new = f(x_new)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "# Plot ROC curve with smoothing\n",
    "plt.figure(figsize=(12, 8))\n",
    "roc_aucs = []\n",
    "\n",
    "# Compute ROC for each class\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_test_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Smooth the curve\n",
    "    fpr_smooth, tpr_smooth = smooth_curve(fpr, tpr)\n",
    "    \n",
    "    # Store for macro-average calculation\n",
    "    roc_aucs.append((class_label, roc_auc, fpr_smooth, tpr_smooth))\n",
    "    \n",
    "    # Plot smoothed curve for each class\n",
    "    plt.plot(fpr_smooth, tpr_smooth, label=f'Class {class_label} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Macro-average ROC curve\n",
    "try:\n",
    "    # Prepare for macro-average\n",
    "    all_fpr = np.unique(np.concatenate([roc[2] for roc in roc_aucs]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    \n",
    "    # Interpolate and average TPR\n",
    "    for _, _, fpr, tpr in roc_aucs:\n",
    "        mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
    "    mean_tpr /= len(roc_aucs)\n",
    "    \n",
    "    # Compute macro-average AUC\n",
    "    macro_auc = auc(all_fpr, mean_tpr)\n",
    "    \n",
    "    # Plot macro-average ROC curve\n",
    "    plt.plot(all_fpr, mean_tpr, label=f'Macro-average (AUC = {macro_auc:.2f})', \n",
    "             color='black', linestyle='--', linewidth=2)\n",
    "except Exception as e:\n",
    "    print(f\"Error computing macro-average: {e}\")\n",
    "\n",
    "# Plot chance line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Smoothed Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_dict = classification_report(y_initial_test, y_initial_test_pred_tuned)\n",
    "df = pd.DataFrame(classification_report_dict).T \n",
    "df = df.drop('support', axis=1)  \n",
    "df = df.drop('accuracy', axis=0, errors='ignore')  \n",
    "sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt='.2f')\n",
    "plt.set_title('Heatmap for Classification Report')\n",
    "plt.set_ylabel('Classes')\n",
    "plt.set_xlabel('Metrics')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
