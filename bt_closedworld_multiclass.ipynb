{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be building our model for the closed-world experiments to classify 95 monitored websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will first import the dataframes into this notebook. Run either 1 of these blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, run this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Use this with colab\n",
    "print(\"Loading datafile...\")\n",
    "with open('datasets/extracted_features.pkl', 'rb') as f:\n",
    "    extracted_df = pickle.load(f)\n",
    "print (\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using local, run this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Use this for local (change the directory to where the extracted_features.pkl is stored on your local machine)\n",
    "# Load the pickle file\n",
    "print(\"Loading datafile...\")\n",
    "# change this directory to the directory where mon_standard.pkl is stored on your local machine\n",
    "file_path = r'C:\\EWHA\\Term 2\\Machine Learning\\pro\\neurotic_networkers\\extracted_features.pkl' # Jordans local path\n",
    "with open(file_path, 'rb') as f: # Path to extracted_features.pkl in Colab\n",
    "    extracted_df = pickle.load(f)\n",
    "print (\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, since we will only be dealing with the closed world experiments, we will first extract the relevant data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_world_df = extracted_df[extracted_df['label'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_world_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we separate the features and the target. Target will be label which represents the label of the monitored websites. Features will be the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_initial = closed_world_df.drop(columns=['label'])\n",
    "y_initial = closed_world_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Model with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we construct a model using all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_initial_train, X_initial_test, y_initial_train, y_initial_test = train_test_split(\n",
    "    X_initial, y_initial, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "clf_all_features = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=100, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf_all_features.fit(X_initial_train, y_initial_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start memory\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the train set\n",
    "y_train_pred = clf_all_features.predict(X_initial_train)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "# tracking end memory\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "time_taken_all_features_train = end_time - start_time\n",
    "memory_used_all_features_train = memory_after - memory_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_all_features_train, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_all_features_train, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_initial_train, y_train_pred))\n",
    "print(classification_report(y_initial_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start memory\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_initial_pred = clf_all_features.predict(X_initial_test)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "# tracking end memory\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "time_taken_all_features_test = end_time - start_time\n",
    "memory_used_all_features_test = memory_after - memory_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_all_features_test, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_all_features_test, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_initial_test, y_initial_pred))\n",
    "print(classification_report(y_initial_test, y_initial_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X_initial_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using entropy\n",
    "model_1= RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=100, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "model_1.fit(X_initial_train, y_initial_train)\n",
    "feature_imp_1 = pd.Series(model_1.feature_importances_, index=df_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gini\n",
    "model_2= RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=100, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "model_2.fit(X_initial_train, y_initial_train)\n",
    "feature_imp_2 = pd.Series(model_2.feature_importances_, index=df_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df_X.columns\n",
    "entropy_values = feature_imp_1.values\n",
    "gini_values = feature_imp_2.values\n",
    "\n",
    "bar_width = 0.4 \n",
    "y_positions = range(len(feature_names)) \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "#plotting entropy importance\n",
    "plt.barh(\n",
    "    [y - bar_width / 2 for y in y_positions],\n",
    "    entropy_values,\n",
    "    bar_width,\n",
    "    label='Entropy',\n",
    "    color='blue',\n",
    ")\n",
    "\n",
    "# plotting gini improtance\n",
    "plt.barh(\n",
    "    [y + bar_width / 2 for y in y_positions],\n",
    "    gini_values,\n",
    "    bar_width,\n",
    "    label='Gini',\n",
    "    color='orange',\n",
    ")\n",
    "\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.yticks(y_positions, feature_names) \n",
    "plt.title('Visualisation of Feature Importance: Entropy vs Gini')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the extracted features:\n",
    "> **Feature Group 1: Traffic Volume (Absolute)**  \n",
    "> - Feature 1: Number of incoming packets  \n",
    "> - Feature 2: Number of outgoing packets  \n",
    "> - Feature 3: Total number of packets  \n",
    "> \n",
    "> **Feature Group 2: Traffic Volume (Fraction)**\n",
    "> - Feature 1: Number of incoming packets as a fraction of the total number of packets  \n",
    "> - Feature 2: Number of outgoing packets as a fraction of the total number of packets \n",
    "> \n",
    "> **Feature Group 3: Traffic Ordering List**\n",
    "> - Feature 6: Standard deviation of the outgoing packets ordering list  \n",
    "> - Feature 7: Average of the outgoing packets ordering list  \n",
    "> \n",
    "> **Feature Group 4: Traffic concentration** \n",
    "> - Feature 8: Sum of all items in the alternative concentration feature list  \n",
    "> - Feature 9: Average of all items in the alternative concentration feature list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noted that within each of the 4 feature groups, the features are likely to be highly correlated due to their similarity. Furthermore, feature groups 1 and 2 are closely related as well with 1 being an absolute measurement of traffic volume and 2 as the ratio. Hence we will be selecting 2 features from the combination of group 1 and 2, and 1 feature each from group 3 and 4.\n",
    "\n",
    "According to our feature importance analysis, we have selected the features to be\n",
    "1. Feature 2: Number of outgoing packets   \n",
    "2. Feature 3: Total number of packets\n",
    "3. Feature 7: Average of the outgoing packets ordering list \n",
    "4. Feature 8: Sum of all items in the alternative concentration feature list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = closed_world_df[['outgoing_packet_counts', 'total_packet_counts', 'avg_outgoing_order', 'sum_concentration']]\n",
    "y = closed_world_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Model with selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we construct an arbitrary random forest classification model using arbitrarily chosen parameters. This section aims to explore the implementation of the model. These parameters will be tuned in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "clf_selected_features = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=100, min_samples_split=2, max_features=\"sqrt\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf_selected_features.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start memory\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the train set\n",
    "y_train_pred = clf_selected_features.predict(X_train)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "# tracking end memory\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "time_taken_selected_features_train = end_time - start_time\n",
    "memory_used_selected_features_train = memory_after - memory_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_train, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_train, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "# JORDAN ADD THE RAM HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start memory\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_test_pred = clf_selected_features.predict(X_test)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "# tracking end memory\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "time_taken_selected_features_test = end_time - start_time\n",
    "memory_used_selected_features_test = memory_after - memory_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_test, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_test, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "# JORDAN ADD THE RAM HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be using Grid Search to tune our model parameters for our Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameter grid for the Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [10, 15],\n",
    "    'min_samples_leaf': [5, 10],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True],\n",
    "    'criterion': ['gini', 'entropy']           \n",
    "}\n",
    "\n",
    "# Defining the grid search\n",
    "grid = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42), \n",
    "    param_grid=param_grid, \n",
    "    cv=5,  \n",
    "    refit=True, \n",
    "    verbose = 3,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the grid search\n",
    "start_time = time.time()\n",
    "grid.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_taken_grid_search = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the time taken to perform the grid search\n",
    "print(\"Time taken for grid search:\", time_taken_grid_search, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters after grid search\n",
    "print(\"Best parameters found:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start memory\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_train_pred = grid.predict(X_train)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "# tracking end memory\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "time_taken_selected_features_tuned_train = end_time - start_time\n",
    "memory_used_selected_features_tuned_train = memory_after - memory_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_tuned_train, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_tuned_train, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process object to track current process\n",
    "process = psutil.Process(os.getpid())\n",
    "# tracking start memory\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)\n",
    "# tracking start time\n",
    "start_time = time.time()\n",
    "# Make predictions on the test set\n",
    "y_test_pred = grid.predict(X_test)\n",
    "# tracking end time\n",
    "end_time = time.time()\n",
    "# tracking end memory\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "time_taken_selected_features_tuned_test = end_time - start_time\n",
    "memory_used_selected_features_tuned_test = memory_after - memory_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy and other metrics\n",
    "print(\"Memory used:\", memory_used_selected_features_tuned_test, \"MB\")\n",
    "print(\"Time taken to predict:\", time_taken_selected_features_tuned_test, \"seconds\")\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "# JORDAN ADD THE RAM HERE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
